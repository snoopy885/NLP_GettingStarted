{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b2a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "438d8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('nlp-getting-started/test.csv')\n",
    "train_len = len(train)\n",
    "all_data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd1237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c548bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Functions\n",
    "def remove_tag(text):\n",
    "    tag = re.compile(r'@\\S+')\n",
    "    return tag.sub(r'',text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    # http:... / https:... / www... \n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url,'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    # < > / ( )\n",
    "    html = re.compile(r'<[^>]+>|\\([^)]+\\)')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    # ['!','\"','$','%','&',\"'\",'(',')','*',\n",
    "    # '+',',','-','.','/',':',';','<','=',\n",
    "    # '>','?','@','[','\\\\',']','^','_','`',\n",
    "    # '{','|','}','~']\n",
    "    punctuations = list(string.punctuation)\n",
    "    table = str.maketrans('', '', ''.join(punctuations))\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fad3b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Nltk is used to remove stopwords and punktuations.\n",
    "\n",
    "Stopwords are words that are used frequently, but have no 'semantic' meaning,\n",
    "which could affact model training (I mean, negatively).\n",
    "\n",
    "And punctuations like '.', ',' also seems they don't have meaningful effect of interpreting sentences.\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15daf86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['cleaned'] = all_data['text'].apply(lambda x:remove_tag(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_URL(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_html(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_punct(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: x.lower()) # lowering\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: word_tokenize(x)) # split sentence into words list\n",
    "# exclude stop words and make them a sentence again\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: ' '.join([word for word in x if word not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249de62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = all_data[:train_len],all_data[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997cad4f",
   "metadata": {},
   "source": [
    "# Dataset DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58e8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self,df,is_grad,tokenizer):\n",
    "        self.df = df # Pandas.DataFrame\n",
    "        self.is_grad = is_grad # True: train,valid / False: test\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) # number of samples\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx]['text'] # extracting text from each row\n",
    "\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=84, # given to the max_length of tokenized text\n",
    "            return_tensors='pt', # PyTorch\n",
    "            return_attention_mask=True, # We should put it into the model\n",
    "        )\n",
    "\n",
    "        if self.is_grad:\n",
    "            labels = self.df.loc[idx]['target']\n",
    "            # [batch,1,max_len(84)] -> [batch,max_len]\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze(),\n",
    "                    # Our loss_fn wants it to be a \"float\" type\n",
    "                    'labels':torch.tensor(labels,dtype=torch.float).unsqueeze(dim=0)}\n",
    "        else:\n",
    "            # [batch,1,max_len(84)] -> [batch,max_len]\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa3c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "model_name = 'bert-large-uncased' # If possible, use \"bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset = TweetDataset(train_data,True,tokenizer)\n",
    "test_dataset = TweetDataset(test_data,False,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f109895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090 train samples\n",
      "1523 valid samples\n",
      "3263 test samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset)) # train:valid = 8:2\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset,valid_dataset = random_split(train_dataset,[train_size,valid_size])\n",
    "\n",
    "print(f'{len(train_dataset)} train samples')\n",
    "print(f'{len(valid_dataset)} valid samples')\n",
    "print(f'{len(test_dataset)} test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f24101e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "'''\n",
    "DataLoader in torch is useful tools for model to get data \"in Batch\".\n",
    "\n",
    "So, the steps we have to take are like this, \n",
    "\"Load CSV file -> make Dataset(in torch.utils.data) -> make DataLoader\"\n",
    "\n",
    "Please pay attention to the fact that I only \"shuffled\" train set, not valid/test set.\n",
    "'''\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True,pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=32,shuffle=False,pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2138a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can give some info to the model,tokenizer or something\n",
    "instead of using arguments below.\n",
    "\n",
    "But, I prefer to use it because it makes experiment more easier.\n",
    "I mean, there is only one cell I have to change for an experiment\n",
    "making some differences in settings.\n",
    "'''\n",
    "configs = {\n",
    "    'model_name':'bert-large-uncased',\n",
    "    'num_labels':2,\n",
    "    'batch_size':32,\n",
    "    'epochs':4,\n",
    "    'learning_rate':5e-6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc4ce06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Never Detach Tensor during forward\n",
    "class TweetsModel(nn.Module):\n",
    "    '''\n",
    "    To be honest, under the setting like this, there is no need to inherit.\n",
    "    It's because I used \"BertForSequenceClassification\" which has final layer\n",
    "    that is composed of \"hidden size 2\" for binary classification.\n",
    "\n",
    "    So, you can think of this unnecessary inheritance is kind of \"practice\" for myself :)\n",
    "    '''    \n",
    "    def __init__(self,model_name):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        output = self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591a0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is running on..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('GPU is running on..')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print('MPS is running on..')\n",
    "else: \n",
    "    device = 'cpu'\n",
    "    print('CPU is running on..')\n",
    "'''\n",
    "You must check your accelerator setting in the right pannel.\n",
    "'''\n",
    "model = TweetsModel(configs['model_name']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29f62e",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8110036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed34f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                lr=6e-6,\n",
    "                eps=1e-8,\n",
    "                no_deprecation_warning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1938fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "metric = f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fb674",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3c0b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc,os\n",
    "from tqdm.auto import tqdm # visualizing tool for progress\n",
    "\n",
    "# They will be used to pick the best model.pt given to the valid loss\n",
    "best_model_epoch, valid_loss_values = [],[] \n",
    "valid_loss_min = [1] # arbitrary loss I set here\n",
    "def train(model,device,train_dataloader,valid_dataloader,epochs,loss_fn,optimizer,metric):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gc.collect() # memory cleaning\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_step = 0\n",
    "        pbar = tqdm(train_dataloader)\n",
    "\n",
    "        for batch in pbar: # you can also write like \"for batch in tqdm(train_dataloader\"\n",
    "            optimizer.zero_grad() # initialize\n",
    "            train_step += 1\n",
    "\n",
    "            train_input_ids = batch['input_ids'].to(device)\n",
    "            train_attention_mask = batch['attention_mask'].to(device)\n",
    "            train_labels = batch['labels'].squeeze().to(device).long()\n",
    "            \n",
    "            # You can refer to the class \"TweetsModel\" for understand \n",
    "            # what would be logits\n",
    "            logits = model(train_input_ids, train_attention_mask).to(device)\n",
    "            predictions = torch.argmax(logits, dim=1) # get an index from larger one\n",
    "            detached_predictions = predictions.detach().cpu().numpy()\n",
    "            \n",
    "            loss = loss_fn(logits, train_labels)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "            pbar.set_postfix({'train_loss':train_loss/train_step})\n",
    "        pbar.close()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            valid_loss = 0\n",
    "            valid_step = 0\n",
    "            total_valid_score = 0\n",
    "\n",
    "            y_pred = [] # for getting f1_score that is a metric of the competition\n",
    "            y_true = []\n",
    "\n",
    "            pbar = tqdm(valid_dataloader)\n",
    "            for batch in pbar:\n",
    "                valid_step += 1\n",
    "\n",
    "                valid_input_ids = batch['input_ids'].to(device)\n",
    "                valid_attention_mask = batch['attention_mask'].to(device)\n",
    "                valid_labels = batch['labels'].squeeze().to(device).long()\n",
    "\n",
    "                logits = model(valid_input_ids, valid_attention_mask).to(device)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                detached_predictions = predictions.detach().cpu().numpy()\n",
    "                \n",
    "                loss = loss_fn(logits, valid_labels)\n",
    "                valid_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "                y_pred.extend(predictions.cpu().numpy())\n",
    "                y_true.extend(valid_labels.cpu().numpy())\n",
    "\n",
    "            valid_loss /= valid_step\n",
    "            f1 = f1_score(y_true,y_pred)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Score: {f1}')\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Valid_loss: {valid_loss}')\n",
    "\n",
    "            if valid_loss < min(valid_loss_min):\n",
    "                print('model improved!')\n",
    "            else:\n",
    "                print('model not improved')\n",
    "    \n",
    "            torch.save(model.state_dict(), 'epoch'+str(epoch+1)+'_model.pt')\n",
    "            print('save checkpoint!')\n",
    "            valid_loss_min.append(valid_loss)\n",
    "            print(f'valid_loss_min:{min(valid_loss_min)}')\n",
    "\n",
    "        best_model_epoch.append(f'epoch{epoch+1}_model.pt')\n",
    "        valid_loss_values.append(valid_loss)\n",
    "        print('='*100)\n",
    "\n",
    "    select_best_model() # refer to below function\n",
    "    print('Train/Valid Completed!!')\n",
    "    del train_dataloader, valid_dataloader # memory cleaning\n",
    "    gc.collect()\n",
    "\n",
    "def select_best_model():\n",
    "    best_model = best_model_epoch[np.array(valid_loss_values).argmin()]\n",
    "    os.rename(best_model, best_model.split('.pt')[0] + '_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "374ecb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, files in current directory: ['.DS_Store', 'nlp-getting-started', 'Easy.ipynb', '.ipynb_checkpoints', 'Bert.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(f'Before training, files in current directory: {os.listdir()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2a09a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start!\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee952a1214a4cb0887c59962d6ba2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94daac69b5a48d99b2cd34f0c0329a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4] Score: 0.7853577371048253\n",
      "Epoch [1/4] Valid_loss: 0.40585625854631263\n",
      "model improved!\n",
      "save checkpoint!\n",
      "valid_loss_min:0.40585625854631263\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ce26ddedc942eeba9f90bbcc9d8d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8f9c54540e4b2e93f6733093cdb7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4] Score: 0.8124999999999999\n",
      "Epoch [2/4] Valid_loss: 0.3911818473910292\n",
      "model improved!\n",
      "save checkpoint!\n",
      "valid_loss_min:0.3911818473910292\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b00fd7f20b425cbaa5202aa9be9862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed96f64c4742424ead88b843d0df4904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4] Score: 0.8067484662576687\n",
      "Epoch [3/4] Valid_loss: 0.4277460804829995\n",
      "model not improved\n",
      "save checkpoint!\n",
      "valid_loss_min:0.3911818473910292\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da1719963e84e74ac35ea88825c2efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae433b04453407b82f7d0960baba8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/4] Score: 0.8072957969865187\n",
      "Epoch [4/4] Valid_loss: 0.43957429379224777\n",
      "model not improved\n",
      "save checkpoint!\n",
      "valid_loss_min:0.3911818473910292\n",
      "====================================================================================================\n",
      "Train/Valid Completed!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training Start!')\n",
    "print('===============')\n",
    "\n",
    "train(model,\n",
    "    device,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    configs['epochs'],\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    metric)\n",
    "\n",
    "del model,train_dataloader, valid_dataloader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424bc87",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27875b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model,test_dataloader):\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            logits = model(input_ids,attention_mask)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            all_preds.append(logits)\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53114312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model.pt: epoch2_model_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c969e83a8dd426c98dee73e17200116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick up the model.pt written with the best\n",
    "# which has the lowest validation loss through all Epochs.\n",
    "\n",
    "for filename in os.listdir():\n",
    "    if 'best.pt' in filename: \n",
    "        best_pt = filename\n",
    "print(f'Best model.pt: {best_pt}')\n",
    "check_point = torch.load(best_pt)\n",
    "\n",
    "# We have to load a model again because I deleted after training/validation\n",
    "model = TweetsModel(configs['model_name']).to(device)\n",
    "model.to(device)\n",
    "model.load_state_dict(check_point)\n",
    "\n",
    "predictions = inference(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b134eda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       0\n",
       "2         3       0\n",
       "3         9       0\n",
       "4        11       0\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       0\n",
       "3260  10868       0\n",
       "3261  10874       0\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('nlp-getting-started/sample_submission.csv')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f8a1520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.argmax(predictions,axis=2) # 0 or 1\n",
    "sample['target'] = predictions\n",
    "sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47f53aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('submission.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca463502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
